<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Will Whitney">
  <title>Thesis Proposal</title>
  <link type="text/css" rel="stylesheet" href="http://cl.ly/code/192R0z2V0n2b/GitHub2.css">
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  
</head>
<body>
<header>
<h1 class="title">Thesis Proposal</h1>
<h2 class="author">Will Whitney</h2>
</header>
<h1 id="introduction">Introduction</h1>
<p>A defining feature of human intelligence is the ability to apply knowledge and techniques learned on one problem to other problems which have similar components. This ranges from the simplest examples (e.g. playing chess with different-colored pieces than you learned with) to the most complicated (like applying concepts from one domain of mathematics to another). A particular formulation of these problems is referred to as transfer learning.</p>
<p>To date, machine learning algorithms have struggled with transfer learning tasks. In some cases existing techniques perform so poorly as to exhibit &quot;negative transfer&quot;, in which seeing examples across new domains causes the learner to perform worse than it did before <span class="citation" data-cites="pan2010survey">(Pan &amp; Yang, 2010)</span>.</p>
<p>With the recent success of deep learning methods in many fields, efforts have been made to apply deep learning techniques to transfer learning problems. Deep learning is at the deepest level a method for hierarchically extracting good representations from complex data, with the higher levels of a network capturing increasingly abstract representations of the data. As such, deep learning seems naively to be a promising direction for transfer learning; abstract representations of the data should be useful for many related tasks, and the network should be able to simply not use any which are not helpful.</p>
<h2 id="catastrophic-forgetting">Catastrophic forgetting</h2>
<p>This theory has been borne out for simple, highly coupled tasks such as evaluating sentiment of reviews for different categories of products <span class="citation" data-cites="glorot2011domain">(Glorot, Bordes, &amp; Bengio, 2011)</span>. A more wide-ranging survey of deep learning methods for transfer learning shows that some classes of models are able to improve their performance on the original, clean dataset after being shown perturbed or distorted versions of the same data <span class="citation" data-cites="bengio2012deep">(Bengio, 2012)</span>.</p>
<p>However, even small changes in the task result in substantial changes to the optimal features, especially at high levels of the network <span class="citation" data-cites="yosinski2014transferable">(Yosinski, Clune, Bengio, &amp; Lipson, 2014)</span>. This can lead to <em>catastrophic forgetting</em>, in which the network &quot;unlearns&quot; its original task as it trains on a new one. A recent set of experiments <span class="citation" data-cites="goodfellow2013empirical">(Goodfellow, Mirza, Xiao, Courville, &amp; Bengio, 2013)</span> detail the tradeoff curve for performance on one task versus performance on the other task for both similar and dissimilar tasks. They show that for networks trained on two tasks, improvement on one task comes at a cost to performance on another.</p>
<p>The true goal of such a system should be <em>function reuse</em>. Ideally a system for transfer learning would learn a set of distinct functions which it uses to solve a given domain of tasks. Each of these functions would represent a particular transformation of the data relevant to solving the problem. Then when presented with a new domain of tasks, this system would have those functions available and could choose whether or not to use them as appropriate for a given task in the new domain. This reuse would represent shared structure between the two domains.</p>
<p>To achieve this, it will be necessary to draw on recent work in attentional models.</p>
<h2 id="attentional-models">Attentional models</h2>
<p>Recently a very powerful new class of neural networks known as attentional models have shown success on a variety of difficult tasks. The Neural Turing Machine <span class="citation" data-cites="graves2014neural">(Graves, Wayne, &amp; Danihelka, 2014)</span> (NTM) performs complex operations on sequences such as sorting or repeatedly copying by using a differentiable addressing mechanism to read and write to an external memory. DRAW <span class="citation" data-cites="gregor2015draw">(Gregor, Danihelka, Graves, &amp; Wierstra, 2015)</span> uses a differentiable attention system to selectively read from its input image and write to an output canvas, as shown in Fig. 1.</p>
<figure>
<img src="draw_attention_small.png" alt="Figure 1: The DRAW network uses attention to render digits onto a canvas." /><figcaption>Figure 1: The DRAW network uses attention to render digits onto a canvas.</figcaption>
</figure>
<p>These systems work by having a Long Short-Term Memory <span class="citation" data-cites="hochreiter1997long">(Hochreiter &amp; Schmidhuber, 1997)</span> (LSTM) controller which produces a weighting distribution over the cells of data structure it indexes. This weighting for each cell is, with some variations in each system, multiplied by the content of the cell and then summed across cells (when reading) or multiplied by the value to write and then added into the cell (when writing).</p>
<p>This attention system allows the network to sequentially</p>
<ol type="1">
<li>Decide on a region of the data to focus on, then</li>
<li>Read or edit just that region of the data.</li>
</ol>
<p>By letting the network sequentially attend to a particular region of the input, this design gives a small attentional network the same power as an enormous one with fixed connectivity. Furthermore, since such a system can decide from one timestep to the next what part of the data is the most important, it can effectively ignore the less diagnostic components of an input or output (e.g., never look at the background of an image).</p>
<p>In this paper, the author proposes a model for transfer learning which addresses the challenge of reusing whichever subroutines are shared between several different tasks without forgetting those subroutines the tasks do not have in common. The proposed system takes the form of an attentional model over components of the network itself.</p>
<h1 id="model">Model</h1>
<figure>
<img src="controller_network_small.png" alt="Figure 2: The controller and layers of the system. The controller provides weights on each layer as a function of the data. This shows three layers, but there can be many more." /><figcaption>Figure 2: The controller and layers of the system. The controller provides weights on each layer as a function of the data. This shows three layers, but there can be many more.</figcaption>
</figure>
<p>The proposed model generates an output for a particular timestep via the following steps (shown in Fig. 2):</p>
<ol type="1">
<li>The input tensor is fed into the controller</li>
<li>The controller decides which layers are most appropriate for processing this input</li>
<li>The controller outputs a weighting vector reflecting how much output it wants from each of the layers</li>
<li>The input tensor is fed into each layer (in parallel)</li>
<li>The outputs from each layer are multiplied by their respective weights from the controller</li>
<li>The weighted outputs from all the layers are summed together and output. This is the output of the whole network for this timestep.</li>
</ol>
<p>Essentially the idea is that at each timestep, the controller examines the input that it gets, then up- or down-regulates the activities of the various &quot;functions&quot; (single-layer NNs) to best deal with this input. Since the controller is an LSTM, it can store information about the inputs it has received before, meaning that in a time series or language setting it can make weighting decisions contextually.</p>
<p>As this model is differentiable throughout, it can be trained with the standard backpropagation through time (BPTT) algorithm for stochastic gradient descent.</p>
<p>By setting weights over each of the layers in the network, the controller scales not only the output of each layer, but also the error gradient that it receives. This means that in a given timestep, the layers which have very low weights on their output will be nearly unchanged by the learning process. That is, functions which are not used are not forgotten.</p>
<p>In an ordinary feedforward neural network, the only way for the network to prevent learning in a particular node is for it to learn connection strengths very near zero for that node. This takes many training examples, and functionally removes that node from the computation graph.</p>
<p>This system, by comparison, can decide that a set of nodes is or is not relevant on an input-by-input basis.</p>
<h2 id="multi-step-variant">Multi-step variant</h2>
<figure>
<img src="multistep_small.png" alt="Figure 3: A variant of the design for using multiple timesteps (in this case, two) to calculate each output." /><figcaption>Figure 3: A variant of the design for using multiple timesteps (in this case, two) to calculate each output.</figcaption>
</figure>
<p>One obvious question to consider about this model is, &quot;What happens if the correct output function at a timestep is not computable with a linear combination of single-layer networks?&quot; After all, there are functions computable by a polynomial-width network of depth <code>k</code> that require exponential width to compute with a network of depth <code>k-1</code> <span class="citation" data-cites="hastad1986almost">(Hastad, 1986)</span>.</p>
<p>To address this question, the system could be run for a predefined number of steps between outputs. That is,</p>
<ol type="1">
<li>Feed the system the input for &quot;real-world&quot; time <code>t</code> and save the output</li>
<li>Repeat <code>k</code> times:
<ol type="a">
<li>Feed the system its own most recent output</li>
</ol></li>
<li>Take the <code>kth</code> output of the system as its answer for time <code>t</code></li>
<li>Repeat from 1. for time <code>t+1</code></li>
</ol>
<p>This amounts to making the network deeper, in that more layers of computation and nonlinearity lie between the input and the output. This gives it the same computational power of a <code>k</code>-depth model.</p>
<h2 id="function-reuse">Function reuse</h2>
<p>The fundamental problem this model attempts to address is that of <em>function reuse</em>. Given that two tasks require some of the same computation to solve, can we build a system that only has to learn that computation once?</p>
<p>If this system has learned a function in one task which happens to also solve another task, the error gradients propagated to the controller will consistently increase the weight assigned to that function and decrease that assigned to the other functions. But this depends on a particular subroutine of one task being exactly the solution to another.</p>
<p>One result which provides hope for partial function reuse is given by Yosinski et al. <span class="citation" data-cites="yosinski2014transferable">(2014)</span>, who showed that a deep network trained to recognize one domain of objects learned to recognize a new domain of objects far faster than an untrained, randomly-initialized network. So clearly there are domains with substantial overlap in their subroutines.</p>
<p>In order to encourage this behavior, it may be necessary to train the network on multiple domains simultaneously, thus letting the functions &quot;coevolve&quot; on all domains and encouraging separation of shared resources from those that are domain-specific.</p>
<p>Whether or not this happens in practice in an experimental question, but the proposed model provides a framework in which it is possible.</p>
<h2 id="connections-to-mixture-of-experts">Connections to mixture of experts</h2>
<p>This architecture has connections to the mixture of experts model proposed by Jacobs et al. <span class="citation" data-cites="jacobs1991task">(1991)</span>, in which several different task-specific &quot;expert&quot; networks each contribute in linear combination to the output of the overall network. The model proposed in this paper can be thought of as akin to an &quot;iterated mixture of experts&quot;, in which a mixture of experts model is sequentially applied to its own output several times before a final output from the network is produced.</p>
<p>However, this model has two key differences from this &quot;iterated mixture of experts&quot;:</p>
<ol type="1">
<li><strong>The gating network is an LSTM.</strong> This means that the gating network (or controller, in my terminology) can easily learn fixed sequential procedures for certain types of input.</li>
<li><strong>The &quot;expert&quot; networks are extremely simple.</strong> Because the model has memory and is iterated, the expert networks are made as simple as they can be: single layers. This give the controller as much flexibility as possible about how to compose functions instead of just choosing between them.</li>
</ol>
<h1 id="experiments">Experiments</h1>
<h2 id="games">Games</h2>
<p>One compelling domain for transfer learning is simple arcade games such as Pac-Man or Breakout. Models such as the DQN <span class="citation" data-cites="mnih2015human">(Mnih et al., 2015)</span> have been recently shown to perform very well on a subset of these games when trained for one game only, but so far efforts to play multiple games with one network have failed. Even minor perturbations in the visual representation of a game (e.g. changing the colors) require substantial retraining.</p>
<p>The author proposes to test the transfer learning abilities of this system by training it with interleaved examples of two similar games at a time. Pairs of games with different levels of similarity can be constructed by everything from creating minor visual differences in the same game to using different source games (e.g. Breakout versus Space Invaders).</p>
<p>Additionally, it would be interesting to see whether the system can efficiently perform curriculum learning. Curriculum learning is a subset of transfer learning which involves transferring skills learned on easy problems in a domain to harder problems in the same domain. Puzzle games with levels of increasing difficulty, such as Rush Hour, would be an interesting domain for this.</p>
<h1 id="discussion">Discussion</h1>
<p>This paper proposes a system capable of taking on challenges in the domain of transfer learning using a model of functional attention. This model consists of an LSTM controller which allots attention and a set of layers which learn functions. By separating the decision of <em>what to execute</em> from the <em>execution</em>, this model will prevent the problems of catastrophic forgetting and negative transfer while allowing the reuse of subcomputations which are shared between tasks.</p>
<h1 id="references" class="references unnumbered">References</h1>
<p>Bengio, Y. (2012). Deep learning of representations for unsupervised and transfer learning. <em>Unsupervised and Transfer Learning Challenges in Machine Learning</em>, <em>7</em>, 19.</p>
<p>Glorot, X., Bordes, A., &amp; Bengio, Y. (2011). Domain adaptation for large-scale sentiment classification: A deep learning approach. In <em>Proceedings of the 28th international conference on machine learning (iCML-11)</em> (pp. 513–520).</p>
<p>Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., &amp; Bengio, Y. (2013). An empirical investigation of catastrophic forgeting in gradient-based neural networks. <em>ArXiv Preprint ArXiv:1312.6211</em>.</p>
<p>Graves, A., Wayne, G., &amp; Danihelka, I. (2014). Neural turing machines. <em>ArXiv Preprint ArXiv:1410.5401</em>.</p>
<p>Gregor, K., Danihelka, I., Graves, A., &amp; Wierstra, D. (2015). DRAW: A recurrent neural network for image generation. <em>ArXiv Preprint ArXiv:1502.04623</em>.</p>
<p>Hastad, J. (1986). Almost optimal lower bounds for small depth circuits. In <em>Proceedings of the eighteenth annual aCM symposium on theory of computing</em> (pp. 6–20). ACM.</p>
<p>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780.</p>
<p>Jacobs, R. A., Jordan, M. I., &amp; Barto, A. G. (1991). Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. <em>Cognitive Science</em>, <em>15</em>(2), 219–250.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … others. (2015). Human-level control through deep reinforcement learning. <em>Nature</em>, <em>518</em>(7540), 529–533.</p>
<p>Pan, S. J., &amp; Yang, Q. (2010). A survey on transfer learning. <em>Knowledge and Data Engineering, IEEE Transactions on</em>, <em>22</em>(10), 1345–1359.</p>
<p>Yosinski, J., Clune, J., Bengio, Y., &amp; Lipson, H. (2014). How transferable are features in deep neural networks? In <em>Advances in neural information processing systems</em> (pp. 3320–3328).</p>
</body>
</html>
